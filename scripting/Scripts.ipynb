{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b23022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "OPENALEX = \"https://api.openalex.org\"\n",
    "arxiv_source_id = \"https://openalex.org/S4306400194\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39250cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHYSICS_PREFIXES = (\n",
    "    \"physics.\", \"astro-ph\", \"cond-mat\", \"hep-\", \"nucl-\", \"gr-qc\", \"quant-ph\", \"math-ph\", \"nlin\"\n",
    ")\n",
    "BIOLOGY_PREFIX = \"q-bio\"\n",
    "\n",
    "def is_physics(cat : str) -> bool:\n",
    "    return bool(cat) and cat.startswith(PHYSICS_PREFIXES)\n",
    "\n",
    "def is_biology(cat : str) -> bool:\n",
    "    return bool(cat) and cat.startwith(BIOLOGY_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fef61e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use re.compile() to efficiently reuse regex pattern (otherwise python creates a new one each time)\n",
    "\n",
    "#Newer arxiv ids are in the format of YYMM.numbers(version optional)eg 2105.12345\n",
    "NEWSTYLE = re.compile(r\"^\\d{4}\\.\\d{4,5}(v\\d+)?$\")\n",
    "#Old style ids are in format of category(.optional subcategory)/numbers(version optional) eg: cs.AI/0102030\n",
    "OLDSTYLE = re.compile(r\"^[a-z\\-]+(\\.[A-Z]{2})?\\/\\d{7}(v\\d+)?$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def normalize_arxiv_id(aid : str) -> str:\n",
    "    if not isinstance(aid, str):\n",
    "        return \"\"\n",
    "    #Strip any erroneous whitespace, and also returns empty string in case nothing given\n",
    "    aid = (aid or \"\").strip()\n",
    "    #Substitutes the optional version ending with empty string\n",
    "    aid = re.sub(r\"v\\d+$\", \"\", aid)\n",
    "    return aid\n",
    "\n",
    "\n",
    "def is_valid_arxiv_id(aid : str) -> bool:\n",
    "    #Arxiv id must be either new or old style\n",
    "    return bool(NEWSTYLE.match(aid) or OLDSTYLE.match(aid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa91e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function inputs url, and returns raw text parsed as json\n",
    "def get_json(url, params = None, retries = 6, backoff = 1.6):\n",
    "    for attempt in range(retries):\n",
    "        #using requests library to pull website data from url\n",
    "        r = requests.get(url, params = params, timeout = 45)\n",
    "        #status code 200 on successful return\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        #failure codes, wait before trying again\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(backoff**attempt)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "359484a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Arxiv abstract urls are in the format of arxiv.org/abs/alphanumericid. It's the same thing accordingly for pdf pages\n",
    "_ARXIV_ABS_RE = re.compile(r\"arxiv\\.org/abs/([^?#/]+)\", re.IGNORECASE)\n",
    "_ARXIV_PDF_RE = re.compile(r\"arxiv\\.org/pdf/([^?#/]+)\", re.IGNORECASE)\n",
    "\n",
    "#Taking the json from the previous function and extracting the proper arxiv id\n",
    "def extract_arxiv_id_from_work(work):\n",
    "    ids = work.get(\"ids\") or {}\n",
    "    #The path we care about in the OpenAlex hierarchy goes work->locations->pdf_url/landing_page_url->arxiv link\n",
    "    for loc in (work.get(\"locations\") or []):\n",
    "        for key in (\"landing_page_url\", \"pdf_url\"):\n",
    "            u = loc.get(key)\n",
    "            if not u:\n",
    "                continue\n",
    "            #the search function in the re. package checks the entire string if it finds a match of the regex pattern\n",
    "            m = _ARXIV_ABS_RE.search(u) or _ARXIV_PDF_RE.search(u)\n",
    "            if m:\n",
    "                #group takes the first () in the regex which will be the id (also removing any possible 'pdf')\n",
    "                aid = normalize_arxiv_id(m.group(1).replace(\".pdf\", \"\"))\n",
    "                if is_valid_arxiv_id(aid):\n",
    "                    return aid\n",
    "    #In many cases, there will exist papers on OpenAlex (eg: Random Forests) but are not on arXiv. Usually, these papers were\n",
    "    #published in the pre internet days, so no corresponding upload to arXiv were made. For these papers, we'll just return nothing\n",
    "    #and treat them as if they don't exist (since we can't parse them from arXiv)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23cd3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_openalex_arxiv_works_cursor(max_works, mailto=None):\n",
    "    #filter for selecting only sources on OpenAlex that are from arXiv\n",
    "    the_filter = f\"locations.source.id:{arxiv_source_id}\"\n",
    "    #selecting only relevant metadata\n",
    "    select = \",\".join([\n",
    "        \"id\", \"doi\", \"title\", \"publication_year\", \"cited_by_count\",\n",
    "        \"ids\", \"locations\", \"type\"\n",
    "    ])\n",
    "\n",
    "    per_page = 200\n",
    "    cursor = \"*\"\n",
    "    #storing final output here in this format -> [{OpenAlex id: _, doi: _, title: _, year: _, citations: _, arXiv id: _, type: _} ...]\n",
    "    rows = []\n",
    "    #for visuals, how far until completion\n",
    "    pbar = tqdm(total=max_works, desc=\"OpenAlex fetch\")\n",
    "\n",
    "    while len(rows) < max_works:\n",
    "        params = {\n",
    "            \"filter\": the_filter,\n",
    "            \"sort\": \"cited_by_count:desc\",\n",
    "            \"per-page\": per_page,\n",
    "            \"cursor\": cursor,\n",
    "            \"select\": select\n",
    "        }\n",
    "        #using get_json as defined earlier, starting from the OpenAlex API and searching through all the top works\n",
    "        data = get_json(f\"{OPENALEX}/works\", params=params)\n",
    "        results = data.get(\"results\", [])\n",
    "        #end early if miss\n",
    "        if not results:\n",
    "            break\n",
    "        #For each found article:\n",
    "        for w in results:\n",
    "            aid = extract_arxiv_id_from_work(w)\n",
    "            if not aid:\n",
    "                continue\n",
    "            #Saving article metadata into rows\n",
    "            rows.append({\n",
    "                \"openalex_id\": w.get(\"id\"),\n",
    "                \"doi\": w.get(\"doi\") or \"\",\n",
    "                \"title\": w.get(\"title\") or \"\",\n",
    "                \"publication_year\": w.get(\"publication_year\"),\n",
    "                \"cited_by_count\": int(w.get(\"cited_by_count\") or 0),\n",
    "                \"arxiv_id\": aid,\n",
    "                \"type\": w.get(\"type\") or \"\"\n",
    "            })\n",
    "\n",
    "            if len(rows) >= max_works:\n",
    "                break\n",
    "\n",
    "        pbar.update(min(len(results), max_works - pbar.n))\n",
    "        cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "\n",
    "        if not cursor:\n",
    "            break\n",
    "        #make sure to wait to not get timed out by the API\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    pbar.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0adc8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the list of arxiv ids, find the categories of these ids\n",
    "def arxiv_query_categories(arxiv_ids, batch_size = 50):\n",
    "    #output will be a map: ids->category\n",
    "    out = {}\n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(arxiv_ids), desc = \"arXiv category resolution\")\n",
    "    \n",
    "    while i < len (arxiv_ids):\n",
    "        #To avoid bottlenecking the arXiv api, only search in chunks of 50\n",
    "        chunk = arxiv_ids[i:i+batch_size]\n",
    "        #arXiv.Search returns a search object that contains all the ids we want arXiv to query\n",
    "        search = arxiv.Search(id_list = chunk)\n",
    "        #No actual api call is made until .results() is called, results contains all the metadata about the corresponding article\n",
    "        #we want its id, and map to its primary category\n",
    "        for result in search.results():\n",
    "            aid = result.get_short_id()\n",
    "            out[aid] = result.primary_category\n",
    "        \n",
    "        i+=len(chunk)\n",
    "        pbar.update(len(chunk))\n",
    "        #wait to avoid bottlenecking\n",
    "        time.sleep(0.2)\n",
    "    pbar.close()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33d85059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_key(w):\n",
    "    year = w[\"publication_year\"] if w[\"publication_year\"] else 9999\n",
    "    return (-w[\"cited_by_count\"], year, w[\"arxiv_id\"])\n",
    "\n",
    "def save(name, rows, out_prefix):\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(f\"{out_prefix}_{name}.csv\", index=False)\n",
    "    with open(f\"{out_prefix}_{name}.jsonl\", \"w\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "def build_top10k_OpenAlex(oversample_factor = 10, out_prefix = \"Test\"):\n",
    "    target = 1000\n",
    "    fetch_n = target * oversample_factor\n",
    "    works = fetch_openalex_arxiv_works_cursor(fetch_n)\n",
    "    pd.DataFrame(works).to_csv(f\"{out_prefix}_openalex_raw.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91085591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all(out_prefix, csv_name, target):\n",
    "    works = pd.read_csv(f\"{csv_name}\").to_dict(orient = \"records\")\n",
    "    cleaned = []\n",
    "    for w in works: \n",
    "        aid = normalize_arxiv_id(w.get(\"arxiv_id\"))\n",
    "        if not isinstance(aid, str):\n",
    "            continue\n",
    "        if is_valid_arxiv_id(aid):\n",
    "            w[\"arxiv_id\"] = aid\n",
    "            cleaned.append(w)\n",
    "\n",
    "    works = cleaned\n",
    "    arxiv_ids = [w[\"arxiv_id\"] for w in works]\n",
    "\n",
    "    cat_map = arxiv_query_categories(arxiv_ids)\n",
    "    \n",
    "    for w in works:\n",
    "        w[\"arxiv_primary_category\"] = cat_map.get(w[\"arxiv_id\"], \"\")\n",
    "    \n",
    "    physics = [w for w in works if is_physics(w[\"arxiv_primary_category\"])]\n",
    "\n",
    "    \n",
    "    physics.sort(key=sort_key)\n",
    "\n",
    "    physics_top = physics[:target]\n",
    "\n",
    "    save(\"physics\", physics_top, out_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4eb44263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "arXiv category resolution: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "build_all(\"physics_test\", \"Test_openalex_raw.csv\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61a264b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openalex_id\n",
      "doi\n",
      "title\n",
      "publication_year\n",
      "cited_by_count\n",
      "arxiv_id\n",
      "type\n"
     ]
    }
   ],
   "source": [
    "works = pd.read_csv(\"Test_openalex_raw.csv\")\n",
    "for w in works:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169db1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
