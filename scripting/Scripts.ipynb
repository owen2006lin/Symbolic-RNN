{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b23022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from __future__ import annotations\n",
    "import csv\n",
    "import tarfile\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "\n",
    "OPENALEX = \"https://api.openalex.org\"\n",
    "arxiv_source_id = \"https://openalex.org/S4306400194\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39250cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHYSICS_PREFIXES = (\n",
    "    \"physics.\", \"astro-ph\", \"cond-mat\", \"hep-\", \"nucl-\", \"gr-qc\", \"quant-ph\", \"math-ph\", \"nlin\"\n",
    ")\n",
    "BIOLOGY_PREFIX = \"q-bio\"\n",
    "\n",
    "def is_physics(cat : str) -> bool:\n",
    "    return bool(cat) and cat.startswith(PHYSICS_PREFIXES)\n",
    "\n",
    "def is_biology(cat : str) -> bool:\n",
    "    return bool(cat) and cat.startwith(BIOLOGY_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fef61e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use re.compile() to efficiently reuse regex pattern (otherwise python creates a new one each time)\n",
    "\n",
    "#Newer arxiv ids are in the format of YYMM.numbers(version optional)eg 2105.12345\n",
    "NEWSTYLE = re.compile(r\"^\\d{4}\\.\\d{4,5}(v\\d+)?$\")\n",
    "#Old style ids are in format of category(.optional subcategory)/numbers(version optional) eg: cs.AI/0102030\n",
    "OLDSTYLE = re.compile(r\"^[a-z\\-]+(\\.[A-Z]{2})?\\/\\d{7}(v\\d+)?$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def normalize_arxiv_id(aid : str) -> str:\n",
    "    if not isinstance(aid, str):\n",
    "        return \"\"\n",
    "    #Strip any erroneous whitespace, and also returns empty string in case nothing given\n",
    "    aid = (aid or \"\").strip()\n",
    "    #Substitutes the optional version ending with empty string\n",
    "    aid = re.sub(r\"v\\d+$\", \"\", aid)\n",
    "    return aid\n",
    "\n",
    "\n",
    "def is_valid_arxiv_id(aid : str) -> bool:\n",
    "    #Arxiv id must be either new or old style\n",
    "    return bool(NEWSTYLE.match(aid) or OLDSTYLE.match(aid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa91e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function inputs url, and returns raw text parsed as json\n",
    "def get_json(url, params = None, retries = 6, backoff = 1.6):\n",
    "    for attempt in range(retries):\n",
    "        #using requests library to pull website data from url\n",
    "        r = requests.get(url, params = params, timeout = 45)\n",
    "        #status code 200 on successful return\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        #failure codes, wait before trying again\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(backoff**attempt)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359484a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Arxiv abstract urls are in the format of arxiv.org/abs/alphanumericid. It's the same thing accordingly for pdf pages\n",
    "_ARXIV_ABS_RE = re.compile(r\"arxiv\\.org/abs/([^?#/]+)\", re.IGNORECASE)\n",
    "_ARXIV_PDF_RE = re.compile(r\"arxiv\\.org/pdf/([^?#/]+)\", re.IGNORECASE)\n",
    "\n",
    "#Taking the json from the previous function and extracting the proper arxiv id\n",
    "def extract_arxiv_id_from_work(work):\n",
    "    ids = work.get(\"ids\") or {}\n",
    "    #The path we care about in the OpenAlex hierarchy goes work->locations->pdf_url/landing_page_url->arxiv link\n",
    "    for loc in (work.get(\"locations\") or []):\n",
    "        for key in (\"landing_page_url\", \"pdf_url\"):\n",
    "            u = loc.get(key)\n",
    "            if not u:\n",
    "                continue\n",
    "            #the search function in the re. package checks the entire string if it finds a match of the regex pattern\n",
    "            m = _ARXIV_ABS_RE.search(u) or _ARXIV_PDF_RE.search(u)\n",
    "            if m:\n",
    "                #group takes the first () in the regex which will be the id (also removing any possible 'pdf')\n",
    "                aid = normalize_arxiv_id(m.group(1).replace(\".pdf\", \"\"))\n",
    "                if is_valid_arxiv_id(aid):\n",
    "                    return aid\n",
    "    #In many cases, there will exist papers on OpenAlex (eg: Random Forests) but are not on arXiv. Usually, these papers were\n",
    "    #published in the pre internet days, so no corresponding upload to arXiv were made. For these papers, we'll just return nothing\n",
    "    #and treat them as if they don't exist (since we can't parse them from arXiv)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23cd3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_openalex_arxiv_works_cursor(max_works, mailto=None):\n",
    "    #filter for selecting only sources on OpenAlex that are from arXiv\n",
    "    the_filter = f\"locations.source.id:{arxiv_source_id}\"\n",
    "    #selecting only relevant metadata\n",
    "    select = \",\".join([\n",
    "        \"id\", \"doi\", \"title\", \"publication_year\", \"cited_by_count\",\n",
    "        \"ids\", \"locations\", \"type\"\n",
    "    ])\n",
    "\n",
    "    per_page = 200\n",
    "    cursor = \"*\"\n",
    "    #storing final output here in this format -> [{OpenAlex id: _, doi: _, title: _, year: _, citations: _, arXiv id: _, type: _} ...]\n",
    "    rows = []\n",
    "    #for visuals, how far until completion\n",
    "    pbar = tqdm(total=max_works, desc=\"OpenAlex fetch\")\n",
    "\n",
    "    while len(rows) < max_works:\n",
    "        params = {\n",
    "            \"filter\": the_filter,\n",
    "            \"sort\": \"cited_by_count:desc\",\n",
    "            \"per-page\": per_page,\n",
    "            \"cursor\": cursor,\n",
    "            \"select\": select\n",
    "        }\n",
    "        #using get_json as defined earlier, starting from the OpenAlex API and searching through all the top works\n",
    "        data = get_json(f\"{OPENALEX}/works\", params=params)\n",
    "        results = data.get(\"results\", [])\n",
    "        #end early if miss\n",
    "        if not results:\n",
    "            break\n",
    "        #For each found article:\n",
    "        for w in results:\n",
    "            aid = extract_arxiv_id_from_work(w)\n",
    "            if not aid:\n",
    "                continue\n",
    "            #Saving article metadata into rows\n",
    "            rows.append({\n",
    "                \"openalex_id\": w.get(\"id\"),\n",
    "                \"doi\": w.get(\"doi\") or \"\",\n",
    "                \"title\": w.get(\"title\") or \"\",\n",
    "                \"publication_year\": w.get(\"publication_year\"),\n",
    "                \"cited_by_count\": int(w.get(\"cited_by_count\") or 0),\n",
    "                \"arxiv_id\": aid,\n",
    "                \"type\": w.get(\"type\") or \"\"\n",
    "            })\n",
    "\n",
    "            if len(rows) >= max_works:\n",
    "                break\n",
    "\n",
    "        pbar.update(min(len(results), max_works - pbar.n))\n",
    "        cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "\n",
    "        if not cursor:\n",
    "            break\n",
    "        #make sure to wait to not get timed out by the API\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    pbar.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0adc8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the list of arxiv ids, find the categories of these ids\n",
    "def arxiv_query_categories(arxiv_ids, batch_size = 50):\n",
    "    #output will be a map: ids->category\n",
    "    out = {}\n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(arxiv_ids), desc = \"arXiv category resolution\")\n",
    "    \n",
    "    while i < len (arxiv_ids):\n",
    "        #To avoid bottlenecking the arXiv api, only search in chunks of 50\n",
    "        chunk = arxiv_ids[i:i+batch_size]\n",
    "        #arXiv.Search returns a search object that contains all the ids we want arXiv to query\n",
    "        search = arxiv.Search(id_list = chunk)\n",
    "        #No actual api call is made until .results() is called, results contains all the metadata about the corresponding article\n",
    "        #we want its id, and map to its primary category\n",
    "        for result in search.results():\n",
    "            aid = result.get_short_id()\n",
    "            out[aid] = result.primary_category\n",
    "        \n",
    "        i+=len(chunk)\n",
    "        pbar.update(len(chunk))\n",
    "        #wait to avoid bottlenecking\n",
    "        time.sleep(0.2)\n",
    "    pbar.close()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d85059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(w):\n",
    "    year = w[\"publication_year\"] if w[\"publication_year\"] else 9999\n",
    "    return (-w[\"cited_by_count\"], year, w[\"arxiv_id\"])\n",
    "\n",
    "def save(name, rows, out_prefix):\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(f\"{out_prefix}_{name}.csv\", index=False)\n",
    "    with open(f\"{out_prefix}_{name}.jsonl\", \"w\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "def build_top10k_OpenAlex(oversample_factor = 10, out_prefix = \"Test\"):\n",
    "    target = 1000\n",
    "    fetch_n = target * oversample_factor\n",
    "    works = fetch_openalex_arxiv_works_cursor(fetch_n)\n",
    "    pd.DataFrame(works).to_csv(f\"{out_prefix}_openalex_raw.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c98b76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to hold a single CSV row in a structured way, arXiv ID links to arXiv source download to extract equations and for category lookup\n",
    "@dataclass\n",
    "class WorkRow:\n",
    "    openalex_id: str\n",
    "    doi: str\n",
    "    title: str\n",
    "    publication_year: int\n",
    "    cited_by_count: int\n",
    "    arxiv_id: Optional[str]\n",
    "    work_type: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13a95d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_openalex_csv_file(csv_path: Path) -> List[WorkRow]:\n",
    "    \"\"\"\n",
    "    Read an OpenAlex CSV file with formatting of build_top10k_OpenAlex():\n",
    "      openalex_url, doi_url, title, year, cited_by_count, arxiv_id, type\n",
    "\n",
    "    Returns WorkRow list. Skips a header row if present.\n",
    "    \"\"\"\n",
    "    rows: List[WorkRow] = []\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        #skip the first line which is just formatting data for the csv: openalex_id,doi,title,publication_year,cited_by_count,arxiv_id,type\n",
    "\n",
    "        next(reader, None)\n",
    "        for parts in reader:\n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            # Ensures parts has exactly 7 columns so append more in case something is missing\n",
    "            while len(parts) < 7:\n",
    "                parts.append(\"\")\n",
    "\n",
    "            openalex_id, doi, title, year, cited_by, arxiv_id, work_type = parts[:7]\n",
    "\n",
    "\n",
    "            year_i = int(year)\n",
    "            cited_i = int(cited_by)\n",
    "\n",
    "\n",
    "            arxiv_id = arxiv_id.strip() or None\n",
    "            #after extracting the data from the csv, create a WorkRow object to append to the output list\n",
    "            rows.append(\n",
    "                WorkRow(\n",
    "                    openalex_id=openalex_id.strip(),\n",
    "                    doi=doi.strip(),\n",
    "                    title=title.strip(),\n",
    "                    publication_year=year_i,\n",
    "                    cited_by_count=cited_i,\n",
    "                    arxiv_id=arxiv_id,\n",
    "                    work_type=work_type.strip(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fa11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_metadata(arxiv_id: str) -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Fetch arXiv metadata for a given id using arxiv.Search.\n",
    "\n",
    "    Returns:\n",
    "      (status, primary_category)\n",
    "      status:\n",
    "        - \"ok\" if found\n",
    "        - \"not_found\" if no result returned\n",
    "        - \"metadata_error\" on exceptions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        result = next(search.results(), None)\n",
    "        if result is None:\n",
    "            return (\"not_found\", None)\n",
    "        primary_category = getattr(result, \"primary_category\", None)\n",
    "        return (\"ok\", primary_category)\n",
    "    except Exception:\n",
    "        return (\"metadata_error\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56f741e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen-G\\AppData\\Local\\Temp\\ipykernel_18100\\3459261263.py:14: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  result = next(search.results(), None)\n",
      "C:\\Users\\Owen-G\\AppData\\Local\\Temp\\ipykernel_18100\\1400416912.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  result = next(search.results(), None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: out/summary.jsonl and out/equations.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def download_and_extract_arxiv_source(arxiv_id: str, out_dir: Path) -> str:\n",
    "    \"\"\"\n",
    "    Download the arXiv source package and extract it into out_dir.\n",
    "\n",
    "    Returns status:\n",
    "      - \"ok\" if extracted\n",
    "      - \"download_error\" if download failed\n",
    "      - \"extract_error\" if tar extraction failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        result = next(search.results(), None)\n",
    "        if result is None:\n",
    "            return \"download_error\"\n",
    "        tar_path = Path(result.download_source(dirpath=str(out_dir)))\n",
    "    except Exception:\n",
    "        return \"download_error\"\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:*\") as tf:\n",
    "            tf.extractall(path=out_dir)\n",
    "        return \"ok\"\n",
    "    except Exception:\n",
    "        return \"extract_error\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) File classification + TeX collection\n",
    "# =========================\n",
    "\n",
    "def is_probably_binary_file(path: Path, sample_bytes: int = 4096) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic to detect binary files:\n",
    "      - contains NUL byte\n",
    "      - too many control characters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = path.read_bytes()[:sample_bytes]\n",
    "    except Exception:\n",
    "        return True\n",
    "    if b\"\\x00\" in data:\n",
    "        return True\n",
    "    nontext = sum(1 for b in data if b < 9 or (b > 13 and b < 32))\n",
    "    return nontext > 0.30 * max(1, len(data))\n",
    "\n",
    "\n",
    "def collect_tex_files(root_dir: Path) -> Tuple[List[Path], int, int]:\n",
    "    \"\"\"\n",
    "    Recursively scan root_dir and return:\n",
    "      - tex_files: list of .tex files\n",
    "      - skipped_binary: count of binary-like files\n",
    "      - skipped_notlatex: count of non-binary but non-.tex files\n",
    "    \"\"\"\n",
    "    tex_files: List[Path] = []\n",
    "    skipped_binary = 0\n",
    "    skipped_notlatex = 0\n",
    "\n",
    "    # common binary extensions (fast skip)\n",
    "    binary_exts = {\".pdf\", \".png\", \".jpg\", \".jpeg\", \".gif\", \".eps\", \".zip\", \".gz\", \".tar\"}\n",
    "\n",
    "    for p in root_dir.rglob(\"*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "\n",
    "        if p.suffix.lower() in binary_exts:\n",
    "            skipped_binary += 1\n",
    "            continue\n",
    "\n",
    "        if is_probably_binary_file(p):\n",
    "            skipped_binary += 1\n",
    "            continue\n",
    "\n",
    "        if p.suffix.lower() == \".tex\":\n",
    "            tex_files.append(p)\n",
    "        else:\n",
    "            skipped_notlatex += 1\n",
    "\n",
    "    return tex_files, skipped_binary, skipped_notlatex\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Equation extraction\n",
    "# =========================\n",
    "\n",
    "EQUATION_ENVS = [\n",
    "    \"equation\", \"equation*\", \"align\", \"align*\", \"gather\", \"gather*\",\n",
    "    \"multline\", \"multline*\", \"eqnarray\", \"eqnarray*\", \"flalign\", \"flalign*\",\n",
    "    \"alignat\", \"alignat*\"\n",
    "]\n",
    "\n",
    "ENV_PATTERN = re.compile(\n",
    "    r\"\\\\begin\\{(\" + \"|\".join(re.escape(e) for e in EQUATION_ENVS) + r\")\\}(.*?)\\\\end\\{\\1\\}\",\n",
    "    re.DOTALL\n",
    ")\n",
    "BRACKET_DISPLAY_PATTERN = re.compile(r\"\\\\\\[(.*?)\\\\\\]\", re.DOTALL)\n",
    "DOLLAR_DISPLAY_PATTERN = re.compile(r\"\\$\\$(.*?)\\$\\$\", re.DOTALL)\n",
    "\n",
    "\n",
    "def strip_latex_comments(tex: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove LaTeX comments (%) while preserving escaped \\\\%.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"(?<!\\\\)%.*\", \"\", tex)\n",
    "\n",
    "\n",
    "def extract_equations_from_tex(tex: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract equation blocks from a TeX document.\n",
    "\n",
    "    Returns list of (kind, latex_block), where kind is one of:\n",
    "      - environment:<name>\n",
    "      - display:\\\\[...\\\\]\n",
    "      - display:$$...$$\n",
    "\n",
    "    Inline $...$ is NOT extracted by default (too noisy).\n",
    "    \"\"\"\n",
    "    tex = strip_latex_comments(tex)\n",
    "    found: List[Tuple[str, str]] = []\n",
    "\n",
    "    for m in ENV_PATTERN.finditer(tex):\n",
    "        env = m.group(1)\n",
    "        found.append((f\"environment:{env}\", m.group(0).strip()))\n",
    "\n",
    "    for m in BRACKET_DISPLAY_PATTERN.finditer(tex):\n",
    "        found.append((\"display:\\\\[...\\\\]\", m.group(0).strip()))\n",
    "\n",
    "    for m in DOLLAR_DISPLAY_PATTERN.finditer(tex):\n",
    "        found.append((\"display:$$...$$\", m.group(0).strip()))\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Process a single row -> summary + equations\n",
    "# =========================\n",
    "\n",
    "def process_work_row(\n",
    "    row: WorkRow,\n",
    "    keep_equations: bool,\n",
    ") -> Tuple[dict, List[dict]]:\n",
    "    \"\"\"\n",
    "    For one WorkRow (must have arxiv_id):\n",
    "      - fetch metadata (category)\n",
    "      - download+extract source\n",
    "      - collect .tex files\n",
    "      - extract equations\n",
    "      - return:\n",
    "          summary_record, equation_records\n",
    "\n",
    "    IMPORTANT:\n",
    "      Every returned record includes cited_by_count and arxiv_primary_category (may be None).\n",
    "    \"\"\"\n",
    "    arxiv_id = normalize_arxiv_id(row.arxiv_id or \"\")\n",
    "\n",
    "    # Ensure required fields always present in output\n",
    "    base_summary = {\n",
    "        \"arxiv_id\": arxiv_id if arxiv_id else None,\n",
    "        \"cited_by_count\": row.cited_by_count,\n",
    "        \"arxiv_primary_category\": None,  # will fill if possible\n",
    "        \"status\": \"init\",\n",
    "        \"tex_files\": 0,\n",
    "        \"skipped_binary\": 0,\n",
    "        \"skipped_notlatex\": 0,\n",
    "        \"equations\": 0,\n",
    "    }\n",
    "\n",
    "    equation_records: List[dict] = []\n",
    "\n",
    "    if not arxiv_id or not is_valid_arxiv_id(arxiv_id):\n",
    "        base_summary[\"status\"] = \"not_arxiv\"\n",
    "        return base_summary, equation_records\n",
    "\n",
    "    # Metadata (category)\n",
    "    meta_status, primary_cat = fetch_arxiv_metadata(arxiv_id)\n",
    "    base_summary[\"arxiv_primary_category\"] = primary_cat\n",
    "\n",
    "    if meta_status != \"ok\":\n",
    "        # We *still* try to download sources; category may remain None.\n",
    "        # But status should reflect metadata problem only if download succeeds.\n",
    "        pass\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        tmp_dir = Path(tmp)\n",
    "\n",
    "        dl_status = download_and_extract_arxiv_source(arxiv_id, tmp_dir)\n",
    "        if dl_status != \"ok\":\n",
    "            base_summary[\"status\"] = dl_status if meta_status == \"ok\" else f\"{meta_status}+{dl_status}\"\n",
    "            return base_summary, equation_records\n",
    "\n",
    "        tex_files, skipped_binary, skipped_notlatex = collect_tex_files(tmp_dir)\n",
    "\n",
    "        eq_count = 0\n",
    "        for tex_path in tex_files:\n",
    "            try:\n",
    "                content = tex_path.read_text(errors=\"ignore\")\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            extracted = extract_equations_from_tex(content)\n",
    "            for idx, (kind, latex_block) in enumerate(extracted):\n",
    "                eq_count += 1\n",
    "                if keep_equations:\n",
    "                    equation_records.append({\n",
    "                        \"arxiv_id\": arxiv_id,\n",
    "                        \"cited_by_count\": row.cited_by_count,\n",
    "                        \"arxiv_primary_category\": primary_cat,\n",
    "                        \"file\": str(tex_path.relative_to(tmp_dir)),\n",
    "                        \"kind\": kind,\n",
    "                        \"index\": idx,\n",
    "                        \"latex\": latex_block,\n",
    "                    })\n",
    "\n",
    "        base_summary.update({\n",
    "            \"status\": \"ok\" if meta_status == \"ok\" else f\"{meta_status}+ok\",\n",
    "            \"tex_files\": len(tex_files),\n",
    "            \"skipped_binary\": skipped_binary,\n",
    "            \"skipped_notlatex\": skipped_notlatex,\n",
    "            \"equations\": eq_count,\n",
    "        })\n",
    "\n",
    "        return base_summary, equation_records\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Batch driver: CSV file -> JSONL outputs\n",
    "# =========================\n",
    "\n",
    "def build_equation_dataset_from_openalex_csv_file(\n",
    "    csv_path: Path,\n",
    "    out_summary_jsonl: Path,\n",
    "    out_equations_jsonl: Optional[Path] = None,\n",
    "    max_papers: Optional[int] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "      - read OpenAlex CSV file\n",
    "      - filter rows with arxiv_id\n",
    "      - process each row\n",
    "      - write JSONL:\n",
    "          summary.jsonl (required)\n",
    "          equations.jsonl (optional)\n",
    "    \"\"\"\n",
    "    rows = read_openalex_csv_file(csv_path)\n",
    "    rows = [r for r in rows if r.arxiv_id]\n",
    "\n",
    "    if max_papers is not None:\n",
    "        rows = rows[:max_papers]\n",
    "\n",
    "    out_summary_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_equations_jsonl is not None:\n",
    "        out_equations_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with out_summary_jsonl.open(\"w\", encoding=\"utf-8\") as fsum:\n",
    "        feq = out_equations_jsonl.open(\"w\", encoding=\"utf-8\") if out_equations_jsonl else None\n",
    "        try:\n",
    "            for row in rows:\n",
    "                summary, eqs = process_work_row(row, keep_equations=(feq is not None))\n",
    "                # Guarantee required fields\n",
    "                assert \"cited_by_count\" in summary\n",
    "                assert \"arxiv_primary_category\" in summary\n",
    "\n",
    "                fsum.write(json.dumps(summary, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                if feq is not None:\n",
    "                    for rec in eqs:\n",
    "                        # Also guarantee required fields per equation record\n",
    "                        assert \"cited_by_count\" in rec\n",
    "                        assert \"arxiv_primary_category\" in rec\n",
    "                        feq.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        finally:\n",
    "            if feq is not None:\n",
    "                feq.close()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Example usage\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = Path(\"Test_openalex_raw.csv\")\n",
    "\n",
    "    build_equation_dataset_from_openalex_csv_file(\n",
    "        csv_path=csv_path,\n",
    "        out_summary_jsonl=Path(\"out/summary.jsonl\"),\n",
    "        out_equations_jsonl=Path(\"out/equations.jsonl\"),\n",
    "        max_papers=50,  # remove/None for full run\n",
    "    )\n",
    "\n",
    "    print(\"Done: out/summary.jsonl and out/equations.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51be3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
