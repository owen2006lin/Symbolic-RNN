{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b23022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "OPENALEX = \"https://api.openalex.org\"\n",
    "arxiv_source_id = \"https://openalex.org/S4306400194\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39250cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHYSICS_PREFIXES = (\n",
    "    \"physics.\", \"astro-ph\", \"cond-mat\", \"hep-\", \"nucl-\", \"gr-qc\", \"quant-ph\", \"math-ph\", \"nlin\"\n",
    ")\n",
    "BIOLOGY_PREFIX = \"q-bio\"\n",
    "\n",
    "def is_physics(cat : str) -> bool:\n",
    "    return bool(cat) and cat.startswith(PHYSICS_PREFIXES)\n",
    "\n",
    "def is_biology(cat : str) -> bool:\n",
    "    return bool(cat) and cat.startwith(BIOLOGY_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef61e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use re.compile() to efficiently reuse regex pattern (otherwise python creates a new one each time)\n",
    "\n",
    "#Newer arxiv ids are in the format of YYMM.numbers(version optional)eg 2105.12345\n",
    "NEWSTYLE = re.compile(r\"^\\d{4}\\.\\d{4,5}(v\\d+)?$\")\n",
    "#Old style ids are in format of category(.optional subcategory)/numbers(version optional) eg: cs.AI/0102030\n",
    "OLDSTYLE = re.compile(r\"^[a-z\\-]+(\\.[A-Z]{2})?\\/\\d{7}(v\\d+)?$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def normalize_arxiv_id(aid : str) -> str:\n",
    "    #Strip any erroneous whitespace, and also returns empty string in case nothing given\n",
    "    aid = (aid or \"\").strip()\n",
    "    #Substitutes the optional version ending with empty string\n",
    "    aid = re.sub(r\"v\\d+$\", \"\", aid)\n",
    "    return aid\n",
    "\n",
    "def is_valid_arxiv_id(aid : str) -> bool:\n",
    "    #Arxiv id must be either new or old style\n",
    "    return bool(NEWSTYLE.match(aid) or OLDSTYLE.match(aid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa91e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function inputs url, and returns raw text parsed as json\n",
    "def get_json(url, params = None, retries = 6, backoff = 1.6):\n",
    "    for attempt in range(retries):\n",
    "        #using requests library to pull website data from url\n",
    "        r = requests.get(url, params = params, timeout = 45)\n",
    "        #status code 200 on successful return\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        #failure codes, wait before trying again\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            time.sleep(backoff**attempt)\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359484a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Arxiv abstract urls are in the format of arxiv.org/abs/alphanumericid. It's the same thing accordingly for pdf pages\n",
    "_ARXIV_ABS_RE = re.compile(r\"arxiv\\.org/abs/([^?#/]+)\", re.IGNORECASE)\n",
    "_ARXIV_PDF_RE = re.compile(r\"arxiv\\.org/pdf/([^?#/]+)\", re.IGNORECASE)\n",
    "\n",
    "#Taking the json from the previous function and extracting the proper arxiv id\n",
    "def extract_arxiv_id_from_work(work):\n",
    "    ids = work.get(\"ids\") or {}\n",
    "    #The path we care about in the OpenAlex hierarchy goes work->locations->pdf_url/landing_page_url->arxiv link\n",
    "    for loc in (work.get(\"locations\") or []):\n",
    "        for key in (\"landing_page_url\", \"pdf_url\"):\n",
    "            u = loc.get(key)\n",
    "            if not u:\n",
    "                continue\n",
    "            #the search function in the re. package checks the entire string if it finds a match of the regex pattern\n",
    "            m = _ARXIV_ABS_RE.search(u) or _ARXIV_PDF_RE.search(u)\n",
    "            if m:\n",
    "                #group takes the first () in the regex which will be the id (also removing any possible 'pdf')\n",
    "                aid = normalize_arxiv_id(m.group(1).replace(\".pdf\", \"\"))\n",
    "                if is_valid_arxiv_id(aid):\n",
    "                    return aid\n",
    "    #In many cases, there will exist papers on OpenAlex (eg: Random Forests) but are not on arXiv. Usually, these papers were\n",
    "    #published in the pre internet days, so no corresponding upload to arXiv were made. For these papers, we'll just return nothing\n",
    "    #and treat them as if they don't exist (since we can't parse them from arXiv)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23cd3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_openalex_arxiv_works_cursor(max_works, mailto=None):\n",
    "    #filter for selecting only sources on OpenAlex that are from arXiv\n",
    "    the_filter = f\"locations.source.id:{arxiv_source_id}\"\n",
    "    #selecting only relevant metadata\n",
    "    select = \",\".join([\n",
    "        \"id\", \"doi\", \"title\", \"publication_year\", \"cited_by_count\",\n",
    "        \"ids\", \"locations\", \"type\"\n",
    "    ])\n",
    "\n",
    "    per_page = 200\n",
    "    cursor = \"*\"\n",
    "    #storing final output here in this format -> [{OpenAlex id: _, doi: _, title: _, year: _, citations: _, arXiv id: _, type: _} ...]\n",
    "    rows = []\n",
    "    #for visuals, how far until completion\n",
    "    pbar = tqdm(total=max_works, desc=\"OpenAlex fetch\")\n",
    "\n",
    "    while len(rows) < max_works:\n",
    "        params = {\n",
    "            \"filter\": the_filter,\n",
    "            \"sort\": \"cited_by_count:desc\",\n",
    "            \"per-page\": per_page,\n",
    "            \"cursor\": cursor,\n",
    "            \"select\": select\n",
    "        }\n",
    "        #using get_json as defined earlier, starting from the OpenAlex API and searching through all the top works\n",
    "        data = get_json(f\"{OPENALEX}/works\", params=params)\n",
    "        results = data.get(\"results\", [])\n",
    "        #end early if miss\n",
    "        if not results:\n",
    "            break\n",
    "        #For each found article:\n",
    "        for w in results:\n",
    "            aid = extract_arxiv_id_from_work(w)\n",
    "            if not aid:\n",
    "                continue\n",
    "            #Saving article metadata into rows\n",
    "            rows.append({\n",
    "                \"openalex_id\": w.get(\"id\"),\n",
    "                \"doi\": w.get(\"doi\") or \"\",\n",
    "                \"title\": w.get(\"title\") or \"\",\n",
    "                \"publication_year\": w.get(\"publication_year\"),\n",
    "                \"cited_by_count\": int(w.get(\"cited_by_count\") or 0),\n",
    "                \"arxiv_id\": aid,\n",
    "                \"type\": w.get(\"type\") or \"\"\n",
    "            })\n",
    "\n",
    "            if len(rows) >= max_works:\n",
    "                break\n",
    "\n",
    "        pbar.update(min(len(results), max_works - pbar.n))\n",
    "        cursor = data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "\n",
    "        if not cursor:\n",
    "            break\n",
    "        #make sure to wait to not get timed out by the API\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    pbar.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc8bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
